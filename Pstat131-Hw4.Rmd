---
title: "Pstat 131 Hw 4"
author: "Noe Arambula"
date: '2022-04-28'
output:
  pdf_document:
    toc: true
  html_document:
    toc: true
    toc_float: true
    code_folding: show
---

output: pdf_document: toc: true html_document: toc: true toc_float: true code_folding: show ---

# Loading Libraries

```{r message=FALSE, warning=FALSE}
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(discrim)

tidymodels_prefer()
```

# Read in CSV Data File and Set Seed

```{r}
titanic <- read_csv("titanic.csv")

set.seed(777)
```

# Changing Survived and pclass to Factors

```{r}
titanic$survived =  factor(titanic$survived, levels = c("Yes", "No")) 
# Note can use parse_factor() in order to give a warning when there is a value not in the set

titanic$pclass =  factor(titanic$pclass)

class(titanic$survived)
class(titanic$pclass)

titanic
```

# Q.1 Splitting Data Create A Recipe Like in HW 3

```{r}
titanic_split <- titanic %>% 
  initial_split(strata = survived, prop = 0.8)

titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)

dim(titanic_train)
dim(titanic_test)
```

Each data set has approximately the right number of observations, the training data has 712 obs. which is about 80% of the full data set, which contains 891 observations.

```{r}
# Titanic recipe like in HW 3
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_train) %>% 
  step_impute_linear(age, impute_with = imp_vars(sib_sp)) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(~ starts_with("sex"):age + age:fare)


```

# Question 2

Fold the **training** data. Use *k*-fold cross-validation, with k=10.

```{r}
# Creating tuned recipe

titanic_tuned_rec <- titanic_recipe %>%
  step_poly(pclass, sex, age, sib_sp, parch, fare, degree = tune())  # polynomial regression


#linear regression model
lm_spec <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

titanic_tuned_wf <- workflow() %>%
  add_recipe(titanic_tuned_rec) %>%
  add_model(lm_spec)
```

```{r}
# rsample object of the cross-validation resamples

titanic_folds <- vfold_cv(titanic_train, v = 10) # Here ISLR uses v instead of k but they are interchangeable, common values include 5 or 10
titanic_folds # creates the k-Fold data set
```

```{r}
# tibble with hyperparameter values we are exploring
degree_grid <- grid_regular(degree(range = c(1, 10)), levels = 10)
degree_grid 
```

```{r message=FALSE, warning=FALSE}
# Do k-fold cross validation; tune_grid() fits the models within each fold for each value specified in degree_grid

tune_res <- tune_grid(
  object = titanic_tuned_wf, 
  resamples = titanic_folds, 
  grid = degree_grid
)
```

# Question 3

In your own words, explain what we are doing in Question 2. What is *k*-fold cross-validation? Why should we use it, rather than simply fitting and testing models on the entire training set? If we **did** use the entire training set, what re-sampling method would that be?

**Answer:** K-fold cross-validation is a re-sampling method in order to help evaluate machine learning models by splitting the data into further smaller samples/subsets that we can perform analysis on separately in order to test our models on various amounts of subsets of the data. So , in order to do this we need to create a workflow object with one parameter set for tuning, in part 2 this was the first step I created the recipe and workflow. Next we need to actually split the data into the folds/multiple smaller training sets which we did using vfold_cv. Lastly, we had to create a tibble with all the values we want to test, so in this case the polynomial degrees we wanted to test.

We should use k-fold cross-validation because it is useful when we smaller amount of observations since we have multiple subsets we can test on. It helps reduce selection bias as we test on many subsets with different observations. It is also useful because it helps us see how our model would perform on new data like the testing set before having to use the actual testing set thus we can see trends such as bias, variance, over fitting, etc and we can try and correct it before using it on the actual testing set. It allows us to fit multiple models on multiple subsets to see how they do.

If we used the entire training set that would be a validation approach.

# Question 4

Set up workflows for 3 models:

A logistic regression with the glm engine;

A linear discriminant analysis with the MASS engine;

A quadratic discriminant analysis with the MASS engine.

How many models, total, across all folds, will you be fitting to the data? To answer, think about how many folds there are, and how many models you'll fit to each fold.

**Answer:** I will be fitting 300 models across all folds to the data since there are we are fitting 3 models to 10 folds each, so we have 3 engines so 10x3 = 30

# Logistic Regression Workflow

```{r}
# We will use the recipe created in Question 2 to create workflows

# Logistic regression
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

titanic_log_wf <- workflow() %>%  # log workflow
  add_recipe(titanic_recipe) %>%
  add_model(log_reg)

```

## LDA Workflow

```{r}
# Linear discriminant analysis
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

titanic_lda_wf <- workflow() %>%  # lda workflow
  add_model(lda_mod) %>%
  add_recipe(titanic_recipe)
```

## QDA Workflow

```{r}
# Quadratic discriminant analysis
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

titanic_qda_wf <- workflow() %>%  # qda workflow
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)
```

# Question 5

Fit each of the models created in Question 4 to the folded data.

IMPORTANT: Some models may take a while to run -- anywhere from 3 to 10 minutes. You should NOT re-run these models each time you knit. Instead, run them once, using an R script, and store your results; look into the use of loading and saving. You should still include the code to run them when you knit, but set eval = FALSE in the code chunks.

```{r eval=FALSE}
# logistic regression fit
fit_log <- tune_grid(  
  object = titanic_log_wf, 
  resamples = titanic_folds, 
  grid = degree_grid,
  control = control_resamples(verbose = TRUE))

# tune_grid fits model within each fold for each value in degree grid
# the control option prints out progress which helps with models that take a long time to fit

```

```{r eval=FALSE}
# linear discriminant analysis fit
fit_lda <- fit_resamples(  
  titanic_lda_wf,
  titanic_folds,
  control = control_resamples(verbose = TRUE))

# tune_grid fits model within each fold for each value in degree grid
# the control option prints out progress which helps with models that take a long time to fit


```

```{r eval=FALSE}
# Quadratic discriminant analysis fit
fit_qda <- fit_resamples(  
  titanic_qda_wf,
  titanic_folds,
  control = control_resamples(verbose = TRUE))

# tune_grid fits model within each fold for each value in degree grid
# the control option prints out progress which helps with models that take a long time to fit
save(fit_log, fit_lda, fit_qda, file = "fittedmodels.rda")
```

I saved all the fitted models in an R script that I previously ran to save time and will now just load them in

```{r}
load(file = "fittedmodels.rda")
```

# Question 6

Use `collect_metrics()` to print the mean and standard errors of the performance metric *accuracy* across all folds for each of the three models.

Decide which of the 3 fitted models has performed the best. Explain why. *(Note: You should consider both the mean accuracy and its standard error.)*

## Logistic Regression

```{r}
# Logistic regression 
collect_metrics(fit_log)
```

## Linear Discriminant Analysis

```{r}
# Linear discriminant analysis
collect_metrics(fit_lda,summarize = F) 
# note adding the summarize = F option allows us to see the estimate for each fold
collect_metrics(fit_lda)
```

## Quadratic Discriminant Analysis

```{r}
# Quadratic discriminant analysis
collect_metrics(fit_qda)
```

## Best Model

Now, after this analysis I have chosen the logistic regression model to be the final model. It performed the best because it had the lowest standard error out of all the models and highest mean for accuracy.

Second best model was qda

# Question 7

Now that you've chosen a model, fit your chosen model to the entire training dataset (not to the folds).

```{r}
# Fitting logistic model to entire training data set
final_fit <- fit(titanic_log_wf, titanic_train)

final_fit
```

# Question 8

Finally, with your fitted model, use `predict()`, `bind_cols()`, and `accuracy()` to assess your model's performance on the testing data!

Compare your model's testing accuracy to its average accuracy across folds. Describe what you see.

```{r}
predict(final_fit, new_data = titanic_test, type = "class") %>% 
  bind_cols(titanic_test %>% select(survived)) %>% 
  accuracy(truth = survived, estimate = .pred_class)
```

The model's testing accuracy was 0.8156425 while the average accuracy for the folds was 0.8061228 so it did a bit better on the testing set then on the folds
